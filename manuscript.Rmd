---
title             : "Commentary: 'Responsible Research Assessment II: A specific proposal for hiring and promotion in psychology'"
shorttitle        : "Responsible research assessment"

author: 
  - name          : "Andreas M. Brandmaier"
    affiliation   : "1,2,3"
    corresponding : yes    # Define only one corresponding author
    address       : "Rüdesheimer Str. 50, 14197 Berlin"
    email         : "andreas.brandmaier@medicalschool-berlin.de"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Supervision"
  - name          : "Maximilian Ernst"
    affiliation   : "2,4"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Aaron Peikert"
    affiliation   : "2,3,4"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"

note: |
  A commentary on:
  Gärtner, A., Leising, D., & Schönbrodt, F. D. (2022, November 25). Responsible Research Assessment II: A specific proposal for hiring and promotion in psychology. https://doi.org/10.31234/osf.io/5yexm
  
  Submitted to Meta-Psychology. Participate in open peer review by commenting through hypothes.is directly on this preprint. The full editorial process of all articles under review at Meta-Psychology can be found following this link: https://tinyurl.com/mp-submissions. You will find this preprint by searching for the first author’sname.

affiliation:
  - id            : "1"
    institution   : "MSB Medical School Berlin"
  - id            : "2"
    institution   : "Max Planck Institute for Human Development"
  - id            : "3"
    institution   : "Max Planck UCL Centre for Computational Psychiatry and Ageing Research"
  - id            : "4"
    institution   : "Humboldt-Universität zu Berlin"
    
correspondence: |
  brandmaier@mpib-berlin.mpg.de

#abstract: |
#  TBD 
  
keywords          : "research software, open science, metrics, rigor, impact"
wordcount         : "`r tryCatch(wordcountaddin::word_count(here::here('manuscript.Rmd')))`"

bibliography      : "r-references.bib"

floatsintext      : no
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf
equal_contrib: yes
header-includes: |
  \usepackage{hyperref}
repro:
  packages:
    - papaja
    - here
    - aaronpeikert/repro@5075336
    - benmarwick/wordcountaddin@e075baa
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Based on four principles of a more responsible research assessment in academic hiring and promotion processes [@schonbrodt2022responsible], @gartner2022responsible suggested a concrete evaluation scheme for published manuscripts, reusable data sets, and research software.
We strongly support the increased emphasis on research software as a creditable and commendable scientific contribution.
Why are research software contributions important scientific contributions?
We would like to respond with a quote from the Science Code Manifesto [@manifesto]:
"Software is a cornerstone of science. Without software, twenty-first century science would be impossible."
and
"Software is an essential research product, and the effort to produce, maintain, adapt, and curate code must be recognized."
However, despite the heavy reliance on computational infrastructure, the current academic infrastructure does not adequately incentivize software development and, specifically, good software engineering practice [@baxter2012research].
In line with principle 3 of @schonbrodt2022responsible, we suggest that criteria for research software contributions must capture two major dimensions: rigor and impact.
Impact measures whether the scholarly effort, implementation, and dissemination actually had a visible effect on the field.
Rigor means implementing high standards and best practices for ensuring transparency, correctness, and re-usability of a piece of software.
By setting a high bar of rigor in research software assessment in academic hiring and promotion, we hope to foster the creation of better software and, thus, better science.
From this perspective, we comment on some indicators of the proposed evaluation scheme for research software contributions (Table 3 of @gartner2022responsible).

# Proposed Criteria

## ID 9: Citations/ID 5: Date of first fully functional public version

Computing the citations in relation to age of software seems to be inconsistent with the proposal of @schonbrodt2022responsible, who reminded us that a core principle of the implementation of DORA is to "abandon the use of invalid quantitative metrics of research quality and productivity in hiring and promotion" (p.2).
It is unclear why a citation-based metric for software would be more valid than the equivalent for articles.
In fact, there are several additional shortcomings in relation to software.
We discuss shortcomings of the numerator (number of citations) here and the issues of the denominator (software age) in the next section.


Citations suffer from serious shortcomings when used to evaluate the impact of research software, particularly when used comparatively in the context of hiring and promotion.
For example, citing data analysis packages is much more commonly accepted than citing supporting packages [such as papaja, @R-papaja, used to render this article].

Further, functionality of successful, modular scientific software is ideally reused in other software packages to avoid code duplication and enable faster development of new software.
While we highly encourage reuse from both a software engineering perspective and for scientific progress, it challenges the validity of citation-based metrics for impact.
For example, consider the `NLopt` optimization suite, which counts 1,711 citations on Google scholar at the time of writing.
`NLopt` is a backbone for many scientific packages both because it implements various optimization algorithms but also because it is open-source and can inspire re-implementations of these algorithms.
One such example is the famous `lme4` package [@bates2014fitting] for generalized linear mixed-effects models, that is partly based on `NLopt`.
`lme4` has more than 58,000 citations on Google scholar, yet it is unlikely that researchers will cite the underlying optimization algorithm.
For another example, the `pdc` package [@pdc-paper] offers functions to cluster time series based on one specific algorithm.
The `TSclust` package [@TSclust] is a wrapper package, which imports and makes accessible functionality from the `pdc` package as well as various other clustering approaches, which is very useful from a users' perspective; however, we noticed that researchers now cite `TSclust` instead of `pdc`, which challenges the citation-based assessment of impact.

## ID 6: Date of most recent substantive update

Both ID 5 and 6 are difficult to ascertain because it is not always clear when updates are considered 'substantive' or software 'fully functional'.
To assess active maintenance as an aspect of rigor, we propose to provide a simple checkbox, in which the author indicates whether a scientific software package is actively maintained (e.g., the software has a regular release cycle or an update within the last six months).
In addition, authors have a chance to explain why their active maintenance may be different from these guidelines.

## ID 14: Lines of Code

We discourage the lines of code (LOC) metric to measure effort.
LOC highly depends on programming language, mastery, and personal programming style.
In particular, many LOC may simply mean that a researcher writes inefficient and repetitive code, one of the great sins of programming.
On the contrary, a feature of good software is modularity because it enables reusing functions both inside and outside the project, resulting in fewer LOC.

## ID 7: Contributor Roles and Involvement

We support the standardized assessment of project contributor roles, similar to the Contributor Roles Taxonomy (CRediT; https://credit.niso.org). 
However, we like to point out that the current evaluation schemes yields lower scores for the same effort of the individual researcher if they are part of a larger software project.

## ID 8: License

At present, whether a piece of software is open source is not evaluated in the pre-screening phase.
However, an open license is central for assessing both rigor and (potential) impact, and should be part of the phase I assessment.
Above, we already discussed the different ways research software can have an impact --- not only by direct usage, but also by reusing software in other packages.
Open-source software makes broader impact more likely and is a prerequisite for full transparency and reproducibility [@peikert2021reproducible].
In addition, many aspects of rigor are impossible to evaluate for closed-source software --- for example, whether it is well-tested or bugs have been fixed.
Therefore, we propose to penalize software if it does not adhere to an open-source license (those approved in a review process by the Open Source Initiative (https://opensource.org/licenses) by allowing them only half of the total achievable points.


## ID 17: Reusability Indicator

This is one of only two criteria used in the pre-screening phase of the proposal and therefore is of central importance.
It is also important because it assesses aspects of rigor in software development: documentation, active maintenance, and testing.
However, by incorporating the size of the user base, it confounds *usability* (as an aspect of rigor) with *usage* (as an aspect of impact).
In addition, the criteria for the different proposed categories are not clearly defined.
For example, the difference between "fairly extensive" and "extensive" documentation is unclear.
As a result, this indicator is more of a "gut-feeling" indicator, roughly assessing the "size" of the software project.
Instead, in the following we propose to assess rigor and impact independently as primary aspects of a software contribution.


# Assessing Rigor

<!--In the following, we discuss aspects that we believe should be scored in phase one of the assessment.-->
We propose to use the following aspects as equally weighted indicators of rigor instead of the proposed broader re-usability indicator (item 17 in Table 3 of the proposal).
<!-- some introductory text missing -->
<!-- maybe cite
https://joss.readthedocs.io/en/latest/review_criteria.html
and
https://joss.readthedocs.io/en/latest/review_checklist.html
here -->

## Tests

Tests are essential to discover incorrect functionality, investigate code scalability and reveal poor design choices.
There are a variety of useful tests, such as unit tests of sub components or tests of software functionality at a larger scale [e.g., see the testthat package in R, @wickham2011].
It is possible to quantify aspects of software testing, for example, by assessing code coverage, defined as the percentage of code lines executed during testing.
However, we believe that we should give points for software that promises that major functionality is covered by tests.
Those tests should be automated or at least open-source and reproducible.

## Documentation

Just like for tests, there are different types of documentation.
For example, tutorials showcase software usage with examples, and there is application programming interface (API) documentation for individual functions and classes to enable reusing functions in other software packages.
We propose to identify relevant categories of documentation for research software, e.g., installation instructions, tutorials, API, and community guidelines (also see https://joss.readthedocs.io/en/latest/review_checklist.html), and score the presence of each of them separately.

## Maintenance

Maintaining a software package is often more work than writing it.
This should be reflected in the assessment procedure.
We propose to score two aspects of maintenance separately, maintaining the code base (such as active bug fixing and documenting changes in logs) and maintaining the community (such as providing the possibility to report bugs, feature requests, or support requests via tickets or mailing lists).

# Measuring Impact

Total citation metrics, number of users, downloads per month, GitHub stars and similar may provide a coarse measure for the impact of a software package, even though it is important to note the shortcomings we described above.

We believe that the suggested merit statement is most useful to assess impact of research software and that this should be the primary statement for committee members to evaluate if they are less concerned with the technical aspects of research software development.
In our view, researchers should be requested to indicate at least one (and up to three) research projects that directly benefited from their software contributions.
We believe this to be a fair assessment of the actual impact the specific contribution of the individual researcher had.


# Summary

In sum, we are thankful to @gartner2022responsible for highlighting the importance of research software contributions as scientifically valuable products.
We believe the current proposal should aim to better reflect the distinction between rigor and impact for software, similar to the guidelines proposed to evaluate journal articles.
To this end, we suggest a more fine-grained assessment of rigor, and put emphasis on the merit statement, in which software authors should argue in how far their project impacted other scientific endeavors.
Last, we hope to arrive at an evaluation scheme that incentivizes the development of scientific open-source software.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
