---
title             : "Commentary: 'Responsible Research Assessment II: A specific proposal for hiring and promotion in psychology'"
shorttitle        : "Responsible research assessment"

author: 
  - name          : "Andreas M. Brandmaier"
    affiliation   : "1,2,3"
    corresponding : yes    # Define only one corresponding author
    address       : "Rüdesheimer Str. 50, 14197 Berlin"
    email         : "andreas.brandmaier@medicalschool-berlin.de"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Supervision"
  - name          : "Maximilian Ernst"
    affiliation   : "2,4"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Aaron Peikert"
    affiliation   : "2,3,4"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"

note: |
  A commentary on:
  Gärtner, A., Leising, D., & Schönbrodt, F. D. (2022, November 25). Responsible Research Assessment II: A specific proposal for hiring and promotion in psychology. https://doi.org/10.31234/osf.io/5yexm
  
  To be submitted to: Meta-Psychology

affiliation:
  - id            : "1"
    institution   : "MSB Medical School Berlin"
  - id            : "2"
    institution   : "Max Planck Institute for Human Development"
  - id            : "3"
    institution   : "Max Planck UCL Centre for Computational Psychiatry and Ageing Research"
  - id            : "4"
    institution   : "Humboldt Universität zu Berlin"
    
correspondence: |
  brandmaier@mpib-berlin.mpg.de

#abstract: |
#  TBD 
  
keywords          : "research software, open science, metrics, rigor, impact"
wordcount         : "`r tryCatch(wordcountaddin::word_count(here::here('manuscript.Rmd')))`"

bibliography      : "r-references.bib"

floatsintext      : no
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf

equal_contrib: yes

header-includes: |
  \usepackage{hyperref}
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction
Based on four basic principles of a more responsible research assessment in academic hiring and promotion processes [@schonbrodt2022responsible], @gartner2022responsible suggested a specific evaluation scheme for published manuscripts, reusable datasets, and research software.
We specifically endorse the increased emphasis on research software as recognizable and laudable scientific contribution.
Why are research software contributions important scientific contributions?
We would like to respond with a quote from the Science Code Manifesto (http://sciencecodemanifesto.org/):
"Software is a cornerstone of science. Without software, twenty-first century science would be impossible."
and
"Software is an essential research product, and the effort to produce, maintain, adapt, and curate code must be recognized."
Yet, despite the heavy reliance on computational infrastructure, the current academic infrastructure does not adequately incentivise software development  and, specifically, good software engineering practice that we would expect from a research software engineer [@baxter2012research].
We suggest that criteria for research software contributions must capture two major dimensions: rigor and impact.
Impact measures whether the scholarly effort and the implementation and dissemination actually had a visible effect on the field.  
Rigor means implementing highest standards and best practices for ensuring transparency, correctness and reusability of a piece of software. 
By setting a high bar of rigor in research software assessment in academic hiring and promotion, we hope to foster the creation of better software and thus better science.
From this perspective, we comment on the proposed evaluation scheme for research software contributions (Table 3 of @gartner2022responsible) and propose alternatives to better capture rigor and impact.

<!--While the authors criticize the use of invalid metrics to evaluate publications (Journal Impact Factors, Citations, H-Index), those same metrics (or ad hoc variants thereof) are mainly relied on in their proposal to evaluate research software.
However, they same critisism applies to those metrics in the context of research software evaluation.
We therefore argue to follow their evaluation scheme for publications: In a first stage, rigor should be evaluated, establishing whether the software product passes a certain treshold, and in the second stage, more complicated dimensions, such as impact, can be evaluated.
In the following, we discuss their proposed criteria.
We then discuss rigor in the context of research software, and propose specific criteria to measure it.
At last, we elaborate on the assessment of impact in the context of research software.

-->

# Proposed Criteria

In the following, we comment on individual indicators proposed in Table 3 [@gartner2022responsible].

## ID 5: Date of first fully functional public version

The date of the first fully functional public version serves as a denominator in computing the number of citations (see ID 9) relative to software age.
In our experience, research software often evolves driven by time constraints and the need for specific functionality, not according to feature completeness. 
Thus, it may generally suffice to ask for the age of the software relative to its first public release without any constraints on functionality.

## ID 6: Date of most recent substantive update

We believe that it's not always clear when updates are considered 'substantive'. 
As an important aspect of rigor, we believe that software needs to be actively maintained.
To assess this, we propose to simply provide a checkbox, in which the author indicates whether a scientific software package is actively maintained (e.g., software has a regular release cycle or an update within the last six months).
In addition, authors have a chance to explain why their active maintenance may be different from these guidelines.

## ID 9: Citations

@gartner2022responsible suggest the number of citations to evaluate the impact of a software package.
First, we like to point out an inconsistency of proposing citation-based metrics and the propositions of @schonbrodt2022responsible. who reminded us that a core principle of the implementation of DORA is "abandon the use of invalid quantitative metrics of research quality and productivity in hiring and promotion" (p.2) and instead to focus on individual achievements.
Further, citations suffer from serious shortcomings when used to evaluate the impact of research software, particularly, when used comparatively in the context of hire and promotion.
For example, it is much more commonly accepted to cite data analysis packages than supporting packages (such as papaja [@R-papaja] used to render this article). 
Further, functionality of successful, modular scientific software is ideally reused in other software packages to avoid duplication of code and enable faster development of new software.
While we highly encourage this from both a software engineering perspective and for scientific progress, it challenges the validity of citation-based metrics for impact.
For example, consider the NLopt optimization suite, which counts 1,711 citations on Google scholar at the time of writing.
NLopt is a backbone for many scientific model fitting packages both because it implements various optimization algorithm but also because it is open-source and can inspire re-implementations of these algorithms.
For example, consider the famous lme4 package [@bates2014fitting] for generalized linear mixed-effects models, that is partly based on NLopt.
This package has more than 58,000 citations on Google scholar, yet it is unlikely that researchers will cite the underlying optimization algorithm.
For another example, the pdc package [@pdc-paper] offers functions to cluster time-series based on relative complexity.
<!--As another example, the TSclust package [@TSclust] is a wrapper to several dissimilarity measures for clustering time series. 
Some of us maintain the pdc clustering package [@pdc-paper], which is contained in TSclust and we saw that researchers use the pdc approach but only cite the TSclust package.
-->

## ID 14: Lines of Code

To measure effort, we discourage the lines of code (LOC) metric.
LOC highly depends on programming language, mastery and personal programming style.
In particular, large LOC of a researcher may simply mean that they write inefficient and repetitive code, one of the great sins of programming.
A feature of good software is modularity because it enables reuse of functions both inside and outside the project.

<!--A great example is the CRAN library that enables packages to built upon other packages and reuse their functionality. However, if LOC were a metric that a researcher are compared with, it encourages researchers to copy&paste chunks of code from other packages instead of importing them in their project.
<!--Second, lines of code is an ambiguous metric.
It can be calculated differently depending on whether comments do count or whether physical lines of code or logical lines of code are counted.-->



## ID 7: Contributer Roles and Involvement
<!-- Larger software project, but the same effort of the individual researcher leads to lower scores? -->

## ID 8: License
At present, the question of whether the software is open source is not used in the computation of a threshold in the prescreening phase.
However, we think an open license is central for assessing both rigor and (potentially) increasing impact and should be part of the phase I assessment.
Above, we already discussed the different ways research software can have an impact --- not only by direct usage, but also by re-using software in other packages.
Broader impact in science is more likely with open source software.
In addition, many aspects of rigor are not possible to evaluate for closed-source software --- for example, whether it is well-tested or whether bugs have been fixed.
Therefore, we propose to penalize software if it does not adhere to an open-source licenses (those approved in a review process by the Open Source Initiative (https://opensource.org/licenses) by allowing them only half of the achievable total points. <!--Alternatively, one may indirectly score openness by assessing the indicators of rigor as discussed in the second part of this commentary, where it is only possible to gain points for certain aspects of rigor if the software is open-source.-->

## ID 17: Reusability Indicator

This is one of only two criteria used in the prescreening phase of the proposal, and therefore is of central importance.
This item is important because it assesses aspects of rigor in software development: documentation, active maintenance, and testing.
However, by incorporating the size of the user base, it confounds *usablity* (as an aspect of rigor) with *usage* (as an aspect of impact).
Importantly, the criteria for the different proposed categories are not clearly defined.
For example, the difference between "fairly extensive" and "extensive" documentation is unclear.
As a result, this indicator is more of a "gut-feeling" indicator, roughly assessing the "size" of the software project.
Instead, in the following we propose to assess rigor and impact independently as primary aspects of a software contribution.

# Rigor for software implementations


<!--In the following, we discuss aspects that we believe should be scored in phase one of the assessment.-->
We propose to use the following aspects as equally weighted indicators of rigor in lieu of the proposed coarse reusability indicator (item 17 in Table 3 of the proposal).
<!-- some introductory text missing -->
<!-- maybe cite
https://joss.readthedocs.io/en/latest/review_criteria.html
and
https://joss.readthedocs.io/en/latest/review_checklist.html
here -->

## Tests

Tests are essential to discover incorrect functionality, investigate scalability of code and reveal poor design choices.
There are a variety of useful tests, such as unit tests of subcomponents or tests of software functionality at larger scale (e.g., see the testthat package in R, @wickham2011).
It would be possible to quantify aspects of software testing, for example, by assessing code coverage defined as the percentage of code lines that are executed during testing.
However, we believe that we should give one point for software that promises that major functionality is covered by tests.
<!--They are usually separated into different categories:
*Unit tests* evaluate only small pieces of code, for example specific R-functions.
*Functional Tests* test whether the input of a user interacting with the software leads to the correct output.
There are many more types of tests, and the type of tests appropriate for a specific software product depends on the software itself.
However, we argue it should at least be tested whether the software can be installed on different platforms, and whether the core functionality of the software is executed without errors (*Smoke Test*).
In addition, it might be possible to give higher scores if different types of tests are employed, or to define a minimum of tests types that should be present, for example *unit tests*, *integration tests* and *functional tests*.
Another metric that could be assessed is code coverage, defined as the percentage of code lines that are executed during testing.
However, we would like to note that this metric is to be taken with a grain of salt, as the quality and number of the tests is not taken into account.
Tests should be automated, or at least reproducible.
This rules out closed-source software tests.
-->
## Documentation

Just like for tests, there are different types of documentation.
For example, there are tutorials showcasing the usage of software with examples, and there is application programming interface (API) documentation for individual functions and classes to enable reuse of functions in other software packages.
We propose to identify relevant categories of documentation for research software and score the presence of each of them separately. <!-- maybe propose the ones from joss: Installation Instructions, Tutorials, API, Community Guidelines -->
<!--AB: moved this to maintenance; Another aspect of documentation is the possibility to ask questions to the package authors (see for example the lavaan mailing list).
-->

## Maintenance

Maintaining a software package is often more work than writing it.
This should be reflected in the assessment procedure.
We propose to score two aspects of maintenance separately, maintaining the actual code base (such as active bug fixing, documenting changes in logs), and maintaining the community (such as providing the possibility to report bugs, feature requests, or support requests via tickets or mailing lists).

## Measuring Impact

In our opinion, total citation metrics, number of users, downloads per month, github stars and similar may provide a coarse measure for the impact of a software package, even though it is important to note the shortcomings we described above.

We believe that the suggested merit statement is most useful to assess impact of research software and that this should be the primary statement for committee members to evaluate if they are less concerned with the technical aspects of research software development.

<!--Complementing the quantitative aspect of impact, we suggest to introduce a mandatory qualitative judgement on the impact of the software, which fits with what @gartner2022responsible proposed as indicator "merit / impact statement".-->

In our view, researchers should be requested to indicate at least one (and up to three)  research projects that directly benefited from the software contributions they made.
We believe this is a fair assessment of the real impact no matter whether someone contributed documentation to run certain types of models, whether someone contributed a specific feature that enabled a given research project to use the software, or whether someone is the primary author of the entire package.


# Summary

In sum, we are thankful to  @gartner2022responsible for highlighting the importance of research software contributions as scientifically valuable products.
We believe that the current proposal must aim to better reflect the distinction between rigor and impact for software like similar to what was proposed to evaluate journal articles.
To this end, we emphasize a more fine-grained assessment of rigor and a qualitative statement on impact, in which software authors should argue in how far their project paved the way for other scientific endeavours.
Last, we hope to arrive at an evaluation scheme that incentivises the development of scientific open source software.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
